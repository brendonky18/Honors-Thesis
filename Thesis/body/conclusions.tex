% Conclusion

By now, the potential utility and pedagogical value of hands-on work is common knowledge among educators, and it is common practice to incorporate some form of such work into cybersecurity curricula. 
There is a vast body of both educational and cybersecurity literature specifically focusing on these subjects. Unfortunately, much of the extant research sees this fact as \emph{non plus ultra}; 
they believe that there is nothing further beyond to be explored. 
While is is clear that interactive work can tremendously enhance the learning experience and educational outcomes for students, what's not incontrovertible is the assumption that this is the end-all be-all in terms of the discussion to be had. 
The use of hands-on work is not the edge of the world, it is merely the horizon. 
This is in fact a highly active area of educational research, it simply seems that cybersecurity educators have not gotten the same message. 

Among the available cybersecurity research, there has been a large amount of energy invested into the design of different frameworks. 
However, this work focuses on enhancing various features, such as flexibility, ease-of-use, extensibility, or modularity. 
This work has seen the conclusion that hands-on work can be greatly beneficial, and taken it to mean that any form of hands-on work is good. 
As this research has shown, that assumption is simply not true. 

Not all hands-on work is made equal, and there a wide number of considerations and combinations thereof that can seriously impact learning outcomes. 
This research sought to examine two of those parameters: 
is there a discernable difference between content-based learning, and skills-based learning; 
and is there a difference in outcomes across varying level guidance. 
It then attempted to examine how the possible differences should inform the future design of hands-on work, in order to maximize it's educational utility. 
The research shows that there are strong indications to confirm both are true --- 
especially so for the latter. 
There is a discernable difference between teaching specific factual knowledge, versus developing skills. 
There is also a positive trend in favor of maximal guidance, suggesting that it is the approach most preferred by students, and that it is the approach that yields the most optimal learning outcomes. 

There are also many practical benefits to a maximally guided approach. 
Namely, It ensures that students will spend a larger portion of their time actively engaged in learning, meaning that their time is being utilized most efficiently with respect to the amount of learning that is being achieved. 
It also mitigates the possibility that students will find workarounds to the intended solution, which could detrimentally impact their learning outcomes. 


\section{Limitations}
% small sample size, extrapolating the data reveals serious trends. 
    An unfortunate and obvious limitation of this research is the small sample size $(N=12)$. 
    Despite this, even with such a small data set, it has produced some statistically significant results. 
    Conversely, with such a small dataset there may be some results or trends that would be significant across a larger population.

    Another issue which must be addressed is the question of bias. 
    There are several flaws in the design of the materials, and of the experiment itself which could bias our results.
    Perhaps the clearest example of this is with selection bias, which comes in two forms. 
    This survey was given only to students taking the class CS 561, towards the latter half of the semester. 
    It is quite possible that these students are not representative of the larger population of cybersecurity students.
    The course work in CS 561 is heavily based on prescriptive hands-on work. 
    It is quite likely that students who do not do as well with this approach to learning would have unenrolled from the course.
    If this is true, then the remaining student's preferences would naturally be skewed in favor of hands-on activities, and towards maximal guidance instructions for such activities.
    
    When administering the experiment, participants were chosen on a voluntary basis. 
    Students were informed that this was an experiment about the use of hands-on activities, and that they would have to complete some activities as part of the experiment. 
    Any students who do not prefer hands-on work may have opted to not participate. 
    To partially adjust for this, a small amount of extra credit was offered for any student who participated, regardless of how much of the activities they were able to complete.
    Unfortunately, this could also introduce bias to our results. 
    In offering extra credit, we may be selecting for a certain kind of student --- such as those who are more highly motivated, or those who are doing poorly in the class and could use the grade boost. 

    Zooming out from the course to the university, this may also be a source of bias. 
    The course CS 561 is offered through the University of Massachusetts Amherst.
    There could be certain factors unique to the school of Computer Science, or the university as a whole.
    As an example, while it is difficult to definitively quantify, a frequent complaint of how Computer Science is taught at this university is that has a strong focus on theoretical teaching, at the expense of practical hands-on work. 
    This could possibly create an inflated desire among students to have greater hands-on work, beyond what is actually valuable. 
    
    A final potential source of bias lies in the design of the activity itself. 
    In both activities, students frequently reported being frustrated by technical issues inherent to the implementation of the activity. 
    These frustrations could easily skew the results in favor of increased guidance, because students are looking for solutions to their technical issues instead of looking for guidance on how to solve the activity. 
    This has the knock-on effect of diluting any underlying signals with additional noise, which makes it even more difficult to extract any statistically significant results.

\section{Future Work}
    An obvious vector for future research would then be to revisit this experiment and remedy it's flaws. 
    The technical issues can easily be resolved, as it relies on the software being used to implement the activities. 
    The sources of bias can  be removed by sampling from a larger and more representative population --- such as across the entire population of Computer Science students, at UMass. 
    UMass also works closely with other nearby colleges, offering excellent access to students from other institution, making our sample even more representative of Computer Science students as a whole. 
    Additionally, this research solely focused on two variables specific variables: 
    amount of guidance, and the kind of knowledge being taught, but there are potentially dozens of contributing factors. 

    Despite the limitations of this experiment's results imposed by its structural weakness, there are still some clear indications of potential future research. There is a positive relationship between guidance and student performance, and there is some merit to the notion that content-based and skill-based learning require different considerations. 
    
% This research only controlled for a single variable
% Future research can examine how these activities can be tailored in accordance with all other kinds of factors. 

% bias introduced by the nature of the class being hands-on
% students in the class are likely to have a higher favorability of hands on work because it is an essential part of the coursework. As the experiment was administered towards the end of the semester, it is likely that students who did not enjoy hands-on work would have unenrolled from the course.
% further self selection bias, since students were informed that the activities involved hands-on work.